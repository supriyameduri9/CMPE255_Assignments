{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5xt82V4fqTiknqPT0SLQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyameduri9/CMPE255_Assignments/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBrpv2wjjK5V"
      },
      "source": [
        "Using any data set you want, implement colabs for the following ANN algorithms (see hints)\n",
        "\n",
        "Write proper documentation in read.me file and colabs\n",
        "\n",
        "a) LSH\n",
        "\n",
        "b) exhaustive search\n",
        "\n",
        "c) product quantization\n",
        "\n",
        "d) trees and graphs\n",
        "\n",
        "e) hnsw\n",
        "\n",
        "Hint : (use as motivation - not exact copy paste)\n",
        "\n",
        "https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6 (Links to an external site.)\n",
        "\n",
        "https://github.com/eyaltrabelsi/my-notebooks/blob/master/Lectures/search_in_practice-approximate_nearest_neighbors/Approximate%20Nearest%20Neighbors.ipynb (Links to an external site.)\n",
        "\n",
        "Hint :\n",
        "\n",
        "colabs at : https://github.com/eyaltrabelsi/my-notebooks/tree/master/Lectures/search_in_practice-approximate_nearest_neighbors (Links to an external site.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDN2nEJujVhN"
      },
      "source": [
        "### Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua0WBs2qv-zU",
        "outputId": "d23c2a83-b003-408a-a77e-9069e885f162"
      },
      "source": [
        "pip install lightfm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightfm\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 204 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 215 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 225 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 235 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 245 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 256 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 266 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 276 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 286 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 296 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 307 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 310 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705358 sha256=a9d921fff08bb6f2632d2e35ddaf7c5b6eb3548e8aa4c2503736fd4bc4ff0f2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN7-YCppcCRB",
        "outputId": "d19375ce-1f6b-4dd0-85c5-1b21060d6a50"
      },
      "source": [
        "pip install nmslib==2.0.11"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nmslib==2.0.11 (from versions: 1.6.3, 1.7, 1.7.1, 1.7.2, 1.7.3.1, 1.7.3.2, 1.7.3.3, 1.7.3.4, 1.7.3.6, 1.8, 1.8.1, 2.0.4, 2.0.5, 2.0.6, 2.1.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for nmslib==2.0.11\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqVg9r1aMaDU",
        "outputId": "84737b19-db17-41bb-bed0-29da0c0efa07"
      },
      "source": [
        "pip install faiss-cpu --no-cache"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 5.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.1.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnrW7a1SQLia",
        "outputId": "e7a92c53-5fb4-441d-f409-919af33eb1f2"
      },
      "source": [
        "pip install annoy"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 646 kB 7.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391660 sha256=3ef99d7af8e88002f0e2fbb6ea34f9de48d27af797422983df36383ca506050c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K1RF9BFjd-K"
      },
      "source": [
        "### Import Dependencies\n",
        "\n",
        "Here I am using [Stack Exchange Network](https://making.lyst.com/lightfm/docs/datasets.html) dataset provided by LightFM to apply different ANN algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omdjp-ZMwLWH"
      },
      "source": [
        "from lightfm import LightFM\n",
        "import pickle\n",
        "from lightfm.datasets import fetch_stackexchange\n",
        "import faiss\n",
        "import annoy\n",
        "import nmslib"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvLvRxfXwJYx"
      },
      "source": [
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4k8xrJMJEax"
      },
      "source": [
        "stackexchange = fetch_stackexchange('crossvalidated',\n",
        "                           test_set_fraction=0.1,\n",
        "                           indicator_features=False,\n",
        "                           tag_features=True)\n",
        "train = stackexchange['train']\n",
        "test = stackexchange['test']\n",
        "\n",
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
        "model.fit_partial(train, item_features=stackexchange['item_features'], epochs=20 )\n",
        "\n",
        "item_vectors = stackexchange['item_features'] * model.item_embeddings"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ZSf16YKNXd"
      },
      "source": [
        "with open('stackexchange.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": stackexchange['item_feature_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luGjxBPAlTRV"
      },
      "source": [
        "#### Load the data from pickeled model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59PqqhUAKeiS",
        "outputId": "e05a773a-72aa-41fc-e363-94f6e17e0068"
      },
      "source": [
        "def load_data():\n",
        "    with open('stackexchange.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "vectors = data[\"vector\"]\n",
        "names = data[\"name\"]\n",
        "data"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['bayesian', 'prior', 'elicitation', ..., 'events', 'mutlivariate',\n",
              "        'sample-variance'], dtype='<U50'),\n",
              " 'vector': array([[ 0.16641644,  0.3901509 , -0.6988499 , ...,  0.53172237,\n",
              "          0.26979873,  0.16564623],\n",
              "        [-0.22441788, -0.01032948, -0.04911942, ...,  0.4292133 ,\n",
              "         -0.22123186,  0.20221922],\n",
              "        [ 0.34115985, -0.17385964,  0.17200977, ...,  0.08830049,\n",
              "         -0.18342032, -0.1314393 ],\n",
              "        ...,\n",
              "        [ 0.16750446,  0.21263315, -0.27324823, ...,  0.05092465,\n",
              "         -0.01612114, -0.13511845],\n",
              "        [ 0.17518018, -0.02437438,  0.15058647, ...,  0.1925765 ,\n",
              "         -0.06998837, -0.19130738],\n",
              "        [ 0.19713715,  0.599983  , -0.38426286, ..., -0.36256272,\n",
              "          0.4619656 ,  0.3476233 ]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFlYijwwUOuY"
      },
      "source": [
        "### Exhaustive Search \n",
        "\n",
        "This technique involves comparing each vector with every other vector , usually not very optimal because it will require linear query time to iterate over the entire dataset.\n",
        "\n",
        "Here I used Faiss ( facebook library for efficient similarity search and clustering of dense vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgRjI2DHPJVN"
      },
      "source": [
        "class ExactIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self):\n",
        "        self.index = faiss.IndexFlatL2(self.dimension,)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7B-u2KqPZOU"
      },
      "source": [
        "index = ExactIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpHU_IM2PtGD",
        "outputId": "253da2c4-333b-4d0a-8e3a-04d492acfb69"
      },
      "source": [
        "stack_vector, stack_name = data['vector'][45], data['name'][45]\n",
        "simlar_stack_names = '\\n* '.join(index.query(stack_vector))\n",
        "print(f\"The most similar stack to {stack_name} are:\\n* {simlar_stack_names}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar stack to overdispersion are:\n",
            "* bayesian\n",
            "* prior\n",
            "* elicitation\n",
            "* distributions\n",
            "* normality\n",
            "* software\n",
            "* open-source\n",
            "* statistical-significance\n",
            "* machine-learning\n",
            "* dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWyKhjypQCE-"
      },
      "source": [
        "### Trees and Forest \n",
        "\n",
        "For tree based algorithms we usually construct forests (collection of trees) as their data structure by splitting the dataset into subsets\n",
        "\n",
        "We use annoy library where number_of_trees — the number of binary trees we build, search_k — the number of binary trees we search for each point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huCNBRwtQCtm"
      },
      "source": [
        "class AnnoyIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in range(len(indices) - 1)]\n",
        "        # print(indices)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFiKsyJrQFhR",
        "outputId": "db660c2c-68d3-49e8-820f-e37627326573"
      },
      "source": [
        "index = AnnoyIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwepEwq6QxrG",
        "outputId": "caa4b923-f390-47d8-8c3c-3b72e3776048"
      },
      "source": [
        "stack_vector, stack_name = data['vector'][45], data['name'][45]\n",
        "simlar_stack_names = '\\n* '.join(index.query(stack_vector))\n",
        "print(f\"The most similar stack to {stack_name} are:\\n* {simlar_stack_names}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar stack to overdispersion are:\n",
            "* bayesian\n",
            "* prior\n",
            "* elicitation\n",
            "* distributions\n",
            "* normality\n",
            "* software\n",
            "* open-source\n",
            "* statistical-significance\n",
            "* machine-learning\n",
            "* dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbpmIP3LU0WO"
      },
      "source": [
        "### Product Quantization\n",
        "\n",
        "Since in real time, we deal with large dataset, therefore linear search is an expensive task, hence we use quantization technique where we reduce dataset size by defining a function that fits the data into compact form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYxpDaD3U4FW"
      },
      "source": [
        "class IVPQIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimention)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimention, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        print(indices)\n",
        "        print(distances)\n",
        "        return [self.labels[i] for i in range(len(indices[0]))]\n",
        "        # return [self.labels[i] for i in range(len(indices))]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4RRnb5YU-2c"
      },
      "source": [
        "index = IVPQIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpxWu9v0VBQR",
        "outputId": "3dca38b3-8a3c-4854-9858-8681e26057c2"
      },
      "source": [
        "stack_vector, stack_name = data['vector'][45], data['name'][45]\n",
        "simlar_stack_names = '\\n* '.join(index.query(stack_vector))\n",
        "print(f\"The most similar stack to {stack_name} are:\\n* {simlar_stack_names}\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2030  5045  9524 ... 16214 15880     0]\n",
            " [  250 49521 13369 ... 12005 11353 10352]\n",
            " [ 5813   433     2 ... 14397  7202 45018]\n",
            " ...\n",
            " [ 3600   618  5171 ...  6982  4852  3116]\n",
            " [72358 34145 68510 ... 27352  2294 31515]\n",
            " [11046 13304 12175 ... 24560 15912  7309]]\n",
            "[[2.844661  2.844661  2.844661  ... 2.844661  2.844661  2.844661 ]\n",
            " [1.4752072 1.4752072 1.4752072 ... 1.510464  1.510464  1.510464 ]\n",
            " [4.156625  4.156625  4.156625  ... 4.4509544 4.471764  4.5681314]\n",
            " ...\n",
            " [2.2162719 2.2162719 2.2162719 ... 2.2162719 2.2162719 2.2162719]\n",
            " [2.994725  3.45093   3.45093   ... 4.4976053 4.4976053 4.4976053]\n",
            " [3.897081  3.897081  3.897081  ... 3.897081  3.897081  3.897081 ]]\n",
            "The most similar stack to overdispersion are:\n",
            "* bayesian\n",
            "* prior\n",
            "* elicitation\n",
            "* distributions\n",
            "* normality\n",
            "* software\n",
            "* open-source\n",
            "* statistical-significance\n",
            "* machine-learning\n",
            "* dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c6065w-aB9T"
      },
      "source": [
        "### LSH - Local Sensitive Hashing\n",
        "\n",
        "We use a hash table to map points nearby into the same bucket in this technique. For this we use faiss library, where num_bits — A larger value will give more accurate results, but larger indexes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1PFzlkTaEFK"
      },
      "source": [
        "class LSHIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self, num_bits=8):\n",
        "        self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in range(len(indices[0]))]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73uQEdCjaQaT"
      },
      "source": [
        "index = LSHIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu6rcvQbaYv9"
      },
      "source": [
        "stack_vector, stack_name = data['vector'][45], data['name'][45]\n",
        "simlar_stack_names = '\\n* '.join(index.query(stack_vector))\n",
        "print(f\"The most similar stack to {stack_name} are:\\n* {simlar_stack_names}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdImEftTaDAc"
      },
      "source": [
        "### HSNW - Hierarchical Navigable Small World Graphs\n",
        "\n",
        "In this algorithm, we reduce the search time by finding the average path. This Algorithm follows the below steps\n",
        " we  At each step \n",
        "\n",
        "1.   Start at a random entry point and iteratively traverse the graph.\n",
        "2.   At each step, we find the distances from a query to the neighbors of a current base node and then selects as the next base node the adjacent node that minimizes the distance, while constantly keeping track of the best-discovered neighbors. \n",
        "3. The search is terminated when some stopping condition is met.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd03cUXIcr9H",
        "outputId": "45209bd0-7fcc-4db4-bd44-16c751612402"
      },
      "source": [
        "pip install nmslib"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Installing collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.1 pybind11-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY7mDbN7bfR_"
      },
      "source": [
        "class NMSLIBIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[i] for i in range(len(indices[0]))]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlEf55Abb5Kk"
      },
      "source": [
        "index = NMSLIBIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYQNOCmWdLra",
        "outputId": "59a45108-08dc-4c63-87b9-ec1dc556207a"
      },
      "source": [
        "stack_vector, stack_name = data['vector'][45], data['name'][45]\n",
        "simlar_stack_names = '\\n* '.join(index.query(stack_vector))\n",
        "print(f\"The most similar stack to {stack_name} are:\\n* {simlar_stack_names}\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar stack to overdispersion are:\n",
            "* bayesian\n",
            "* prior\n",
            "* elicitation\n",
            "* distributions\n",
            "* normality\n",
            "* software\n",
            "* open-source\n",
            "* statistical-significance\n",
            "* machine-learning\n",
            "* dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss1mgOHXj6x9"
      },
      "source": [
        "### References\n",
        "\n",
        "1.   https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6\n",
        "2.   https://github.com/lyst/lightfm/blob/master/examples/stackexchange/hybrid_crossvalidated.ipynb\n",
        "\n"
      ]
    }
  ]
}